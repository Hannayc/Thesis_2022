{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e6db83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from tqdm import tqdm_notebook \n",
    "\n",
    "import re\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud\n",
    "from csaps import csaps\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import networkx as nx\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "import community.community_louvain\n",
    "from pyvis.network import Network\n",
    "import operator\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "\n",
    "import tweepy\n",
    "import time\n",
    "import advertools as adv\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.wrappers import LdaMallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cf9df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp_nl = spacy.load(\"nl_core_news_sm\")\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp_des_nl = spacy.load(\"nl_core_news_sm\")\n",
    "nlp_des_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp_des_nl.Defaults.stop_words |= {'brabant','een','twee','my','are','on','once', }\n",
    "\n",
    "nlp_en.Defaults.stop_words |= {'de','der','van','een','nee','dank','en','bron','morge','het','precie','precies',\n",
    "                               'regard','regards','lol','dm','thnks','thnk','lot','op','minute','minutes','hour',\n",
    "                               'hours','day','days','week','weeks','weekend','weekends','month','months','year',\n",
    "                               'years','today','todays','morning','afternoon','pm','tonight','night','evening',\n",
    "                               'tomorrow','end','people','guy','guys','shit','thing','things','w/','bit','ppm',\n",
    "                               'yr','yrs',}\n",
    "\n",
    "nlp_nl.Defaults.stop_words |= {'de','van','een','nee','dank','en','bron','morge','avond','het','dag','dagen','jaar',\n",
    "                               'dm','gister','morgen','weekje','weken','week','maand','maanden','uur','uurtje',\n",
    "                               'minuut','minuten','mensen','weekend','Spui','ur','stuk','luister',}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f620ead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input Twitter credentials\n",
    "consumer_key= \"\"\n",
    "consumer_secret= \"\" \n",
    "access_token=\"\"\n",
    "access_token_secret=\"\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)    \n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "#return all friends and followers' id in two lists \n",
    "def get_friends_and_followers(username_or_id):\n",
    "    \n",
    "    friends_id=api.get_friend_ids(user_id=username_or_id)\n",
    "    followers_id=api.get_follower_ids(user_id=username_or_id)\n",
    "    \n",
    "    return friends_id, followers_id\n",
    "\n",
    "#returns reciprocal friends in a list \n",
    "def setwise_friends_followers_analysis(screen_name, friends_ids, followers_ids):\n",
    "    \n",
    "    friends_ids, followers_ids = set(friends_ids), set(followers_ids)\n",
    "    \n",
    "    print('{0} is following {1}'.format(screen_name, len(friends_ids)))\n",
    "    \n",
    "    print('{0} is being followed by {1}'.format(screen_name, len(followers_ids)))\n",
    "    \n",
    "    print('{0} of {1} are not following {2} back'.format(\n",
    "            len(friends_ids.difference(followers_ids)), \n",
    "            len(friends_ids), screen_name))\n",
    "    \n",
    "    print('{0} of {1} are not being followed back by {2}'.format(\n",
    "            len(followers_ids.difference(friends_ids)), \n",
    "            len(followers_ids), screen_name))\n",
    "    \n",
    "    print('{0} has {1} mutual friends'.format(\n",
    "            screen_name, len(friends_ids.intersection(followers_ids))))\n",
    "    \n",
    "    return list(friends_ids.intersection(followers_ids))\n",
    "\n",
    "#return the follower count of the user's reciprocal friends.\n",
    "def get_your_friends_follower_count(reciprocal_friends):\n",
    "    \n",
    "    #dictionary store information about friend and their follower count\n",
    "    get_info={}\n",
    "    #list to hold mined ids\n",
    "    mined_ids=[]\n",
    "    #list to hold mined follower counts\n",
    "    mined_followers_count=[]\n",
    "    #lists hold final values\n",
    "    ids=[]\n",
    "    followers_count=[]\n",
    "    \n",
    "    #execute when the length of the reciporcal friends list is greater than 100\n",
    "    if len(reciprocal_friends) > 100:\n",
    "        \n",
    "        #tweepy only return the information of 100 followers at a time.\n",
    "        #This code calculates values of 100 per number of followers\n",
    "        #So if one has 1000 followers, the function will be calculated like: 100,200 and so on.\n",
    "        value=len(reciprocal_friends)\n",
    "        iters=int(value/100)\n",
    "        it=-1\n",
    "        vals=[]\n",
    "        for i in range(0,iters):\n",
    "            vals.append([(it+1)*100,(it+2)*100])\n",
    "            it=it+1\n",
    "            \n",
    "        vals.append([iters*100,(iters*100)+(value%100)])\n",
    "         \n",
    "        a=[]   \n",
    "        \n",
    "        #Using the ranges in our vals list, use list to effectively make api calls\n",
    "        for j in range(0,len(vals)):\n",
    "            \n",
    "            #json formated dict is stored in mined lists\n",
    "            mined_ids.extend(api.lookup_users(user_id=reciprocal_friends[vals[j][0]:vals[j][1]]))\n",
    "            mined_followers_count.extend(api.lookup_users(user_id=reciprocal_friends[vals[j][0]:vals[j][1]]))\n",
    "        \n",
    "        #exact target values are stored in this list\n",
    "        for k in range(0,len(mined_ids)):\n",
    "            ids.append(mined_ids[k]._json['id'])\n",
    "            followers_count.append(mined_followers_count[k]._json['followers_count'])\n",
    "              \n",
    "    else:\n",
    "        a=api.lookup_users(user_id=reciprocal_friends[0:100])\n",
    "        for i in range(0,len(a)):\n",
    "            ids.append(str(a[i]._json['id']))\n",
    "            followers_count.append(a[i]._json['followers_count'])\n",
    "    \n",
    "    #convert both lists to a dictionary and return the data\n",
    "    list_to_dict=zip(ids,followers_count)\n",
    "    info=dict(list_to_dict)\n",
    "    \n",
    "    return info\n",
    "\n",
    "#get the top 5 influential friend\n",
    "def get_top_5_friends(info):\n",
    "    \n",
    "    #convert the values received from the previous function into a list and sort it\n",
    "    values=list(info.values())\n",
    "    values.sort()\n",
    "    top_friends_followers_ids=[]\n",
    "    \n",
    "    #top 5 follower counts will be the 5 greatest at the end of the list\n",
    "    follower_counts=values[len(values)-5:len(values)]\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    if len(info) <5:\n",
    "        return info\n",
    "    \n",
    "    #iterate through the keys of the info dict, if the value in the dict equals the follower counts stored \n",
    "    #in the follower_counts list, store it in the top_friends_followers_ids list\n",
    "    while(i<5):\n",
    "        \n",
    "        for element in list(info.keys()):\n",
    "            if info[element]==follower_counts[i]:\n",
    "                top_friends_followers_ids.append(element)\n",
    "        i=i+1\n",
    "        \n",
    "        top_5=zip(top_friends_followers_ids,follower_counts)\n",
    "        top_5=dict(top_5)\n",
    "        \n",
    "    return top_5\n",
    "\n",
    "#function calls all previous functions to generate distance 1 friends with the top 5 highest follower counts.\n",
    "def get_distance_one_friends(username_or_id):\n",
    "    \n",
    "    friends_id, followers_id=get_friends_and_followers(username_or_id)\n",
    "    reciprocal_friends=setwise_friends_followers_analysis(username_or_id, friends_id,followers_id)\n",
    "    info=get_your_friends_follower_count(reciprocal_friends)\n",
    "    top_5=get_top_5_friends(info)\n",
    "    return top_5\n",
    "\n",
    "#use return data of distance 1 friends to find information about top 5 most popular followers from each user.\n",
    "def get_distance_two_friends(username_or_id):\n",
    "    \n",
    "    data=get_distance_one_friends(username_or_id)\n",
    "    information=[]\n",
    "    list_data_keys=list(data.keys())\n",
    "    \n",
    "    for element in list_data_keys:\n",
    "        information.append(get_distance_one_friends(element))\n",
    "        \n",
    "    return information\n",
    "\n",
    "#input list of distance 2 friends to find information about the top 5 most popular followers from each user.\n",
    "def get_distance_three_friends(data):\n",
    "    information=[]\n",
    "    \n",
    "    for element in data:\n",
    "        for i in list(element.keys()):\n",
    "            try:\n",
    "                information.append(get_distance_one_friends(i))\n",
    "            \n",
    "            # sleep for 15 minutes when Tweepy's rate limiterror occured.\n",
    "            except tweepy.TweepError:\n",
    "                time.sleep(60 * 15)\n",
    "                continue\n",
    "    \n",
    "    return information\n",
    "\n",
    "def network(user_id, name, data_1, data_2, data_3):    \n",
    "    rcParams[\"figure.figsize\"]=50,40 \n",
    "    rcParams['axes.titlesize']=70\n",
    "\n",
    "    #instantiate a networkx graph object    \n",
    "    G=nx.MultiGraph()\n",
    "    head=user_id\n",
    "    G.add_node(head)\n",
    "\n",
    "    #converts dictionaries into list format from data points 1,2 and 3\n",
    "    list_data_1=list(data_1.keys())\n",
    "    G.add_nodes_from(list_data_1)\n",
    "    list_data_2=[]\n",
    "\n",
    "    for element in data_2:\n",
    "        for i in element.keys():\n",
    "            list_data_2.append(i)\n",
    "\n",
    "    list_data_3=[]\n",
    "\n",
    "    for element in data_3:\n",
    "        for i in element.keys():\n",
    "            list_data_3.append(i)\n",
    "\n",
    "    #add edges from my account and my top 5 most popular friends\n",
    "    for i in range(0,len(list_data_1)):\n",
    "        G.add_edges_from([(user_id,list_data_1[i])])\n",
    "\n",
    "    #add edges between distance 1 friends and distance 2 friends    \n",
    "    for j in range(0,len(data_2)):\n",
    "        for k in list(data_2[j].keys()):\n",
    "            G.add_edges_from([(list_data_1[j],k)])\n",
    "\n",
    "    #add edges between distance 2 friends and distance 3 friends \n",
    "    for l in range(0,len(data_3)):\n",
    "        for m in list(data_3[l].keys()):\n",
    "            G.add_edges_from([(list_data_2[l],m)])\n",
    "\n",
    "    # Computes the partition of the graph nodes which maximises the modularity using the Louvain heuristics algorithm. \n",
    "    # This is the partition of highest modularity.\n",
    "    part = community.community_louvain.best_partition(G,random_state=10)\n",
    "    # Computes the modularity of a partition of a graph\n",
    "    mod = community.community_louvain.modularity(part,G)\n",
    "\n",
    "    values = [part.get(node) for node in G.nodes()]\n",
    "\n",
    "    plt.title(name+ \"'s Twitter Network\") \n",
    "    pos=nx.spring_layout(G,scale=4)\n",
    "    nx.draw(G, pos, cmap=plt.cm.RdYlBu, node_size=5000, node_color = values, font_size=12, font_weight='bold',with_labels=False)\n",
    "    plt.savefig(name+'.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    n_edges=G.number_of_edges()\n",
    "    diameter=nx.diameter(G)\n",
    "    average_dist=nx.average_shortest_path_length(G)\n",
    "    n_nodes=G.nodes()\n",
    "\n",
    "    print(\"This graph contains {0} distinct nodes with {1} edges. \".format(len(n_nodes),n_edges))\n",
    "    print(\"The Diameter of this network is: {0} \".format(diameter))\n",
    "    print(\"The average distance between each node in the Graph is: {0}\".format(average_dist))\n",
    "    print('The modularity is', mod)\n",
    "    \n",
    "    return part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7f0621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for preprocessing and topic modeling\n",
    "\n",
    "PATH_TO_MALLET = ''#input local path to MALLET\n",
    "\n",
    "MIN_DF = 3 # Only include words that appear in more than 3 tweets\n",
    "MAX_DF = 0.7 # Only include words that appear in less than 70% of the tweets \n",
    "\n",
    "#function for cleaning text\n",
    "def clean_text(tweet):\n",
    "    tweet = BeautifulSoup(tweet).get_text()\n",
    "    tweet=' '.join(re.sub(\"(@[a-zA-Z0-9_]+)|(#[A-Za-z0-9_]+)\", \" \", tweet).split())#remove mention and hashtag\n",
    "    tweet=' '.join(re.sub(\"(RT[a-zA-Z0-9_].\\:)|(RT :)|(RT)\", \" \", tweet).split())#remove retweet indication\n",
    "    tweet=' '.join(re.sub(r\"http\\S+\", \" \", tweet).split())#remove url\n",
    "    tweet=' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", tweet).split())#remove link\n",
    "    tweet=' '.join(re.sub(\"(\\$[0-9]\\S)|(\\S[0-9])|([0-9]\\S)|([0-9]+)\", \" \", tweet).split())#remove numbers \n",
    "    tweet=' '.join(re.sub(\"(\\>)|(\\<)|(\\|)|(\\+)|(\\=)|(\\*)|(\\^)|(\\$)|(\\-)\", \" \", tweet).split())#remove signs\n",
    "    tweet=' '.join(re.sub(\"\\(\\)\", \" \", tweet).split())\n",
    "    tweet=' '.join(re.sub(\"Eric\", \" \", tweet).split())\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "def pre_process(df):\n",
    "    lang=['en','nl']\n",
    "    df['tweet_year'] = pd.DatetimeIndex(df['tweet_created_at']).year\n",
    "    df['dup_text']=df['tweet_full_text']\n",
    "    df['dup_text']=df['dup_text'].str.replace(r'[^\\x00-\\x7F]+', '', regex=True) #remove icons\n",
    "    tweet_text=df.dup_text.values\n",
    "    df['clean_text'] = [clean_text(text) for text in tweet_text]\n",
    "    df=df.loc[df['tweet_lang'].isin(lang)]#keep only English and Dutch tweets\n",
    "    \n",
    "    return df\n",
    "\n",
    "def spacy(df, nlp):\n",
    "    processed_texts = [text for text in tqdm_notebook(nlp.pipe(df['clean_text'], disable=[\"ner\", \"parser\"]),\n",
    "                                                  total=len(df['clean_text']))]\n",
    "\n",
    "\n",
    "    tokenized_texts = [[token.lemma_.lower() for token in text if not token.is_punct \n",
    "                    and not token.is_stop and token.pos_ == 'NOUN'] \n",
    "                   for text in tqdm_notebook(processed_texts)] \n",
    "\n",
    "    df['tokenized_texts']=tokenized_texts\n",
    "\n",
    "    dictionary = Dictionary(df['tokenized_texts'])# Get the vocabulary\n",
    "    dictionary.filter_extremes(no_below=MIN_DF, \n",
    "                           no_above=MAX_DF) # Remove words that appear in more than 70% of the tweets\n",
    "\n",
    "    # Transform a list of words into a bag of words representation. \n",
    "    corpus = [dictionary.doc2bow(text) for text in df['tokenized_texts']]\n",
    "    \n",
    "    return df, dictionary, corpus\n",
    "\n",
    "def spacy_description(df, nlp):\n",
    "    processed_texts = [text for text in tqdm_notebook(nlp.pipe(df['clean_description'], disable=[\"ner\", \"parser\"]),\n",
    "                                                      total=len(df['clean_description']))]\n",
    "\n",
    "\n",
    "    tokenized_texts = [[token.lemma_.lower() for token in text if not token.is_punct \n",
    "                        and not token.is_stop] \n",
    "                       for text in tqdm_notebook(processed_texts)] \n",
    "\n",
    "    df['tokenized_texts']=tokenized_texts\n",
    "\n",
    "    dictionary = Dictionary(df['tokenized_texts']) # Get the vocabulary\n",
    "\n",
    "    # Transform a list of words into a bag of words representation. \n",
    "    corpus = [dictionary.doc2bow(text) for text in df['tokenized_texts']]\n",
    "\n",
    "    return df, dictionary, corpus\n",
    "\n",
    "#for plotting\n",
    "def smooth(x, y, smooth=0.95):\n",
    "    \n",
    "    xs = np.linspace(x[0], x[-1], 150)\n",
    "    ys = csaps(x, y, xs, smooth=smooth)\n",
    "    \n",
    "    return xs, ys\n",
    "\n",
    "#calculate the coherence value for different topic numbers\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(PATH_TO_MALLET, corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model,texts=texts,dictionary=dictionary,coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n",
    "\n",
    "# Show coherence graph for different topic number\n",
    "def topic_coherence(name, coherence_values):\n",
    "\n",
    "    limit=20; start=1; step=3;\n",
    "    x = range(start, limit, step)\n",
    "    ax2 = plt.plot(x, coherence_values)\n",
    "    plt.xlabel(\"Number of Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.title('Coherence score per number of topics \\n'+name)\n",
    "    plt.legend((\"\"), loc='best')\n",
    "    plt.savefig(name+'.png', dpi=600)\n",
    "    plt.show()\n",
    "    # Print the coherence scores\n",
    "    for m, cv in zip(x, coherence_values):\n",
    "        print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))\n",
    "\n",
    "# train the Latent Dirichlet Allocation algorithm\n",
    "def train_LDA(corpus, dictionary, N_TOPICS): \n",
    "    \n",
    "    N_ITERATIONS = 2000 # Number of training iterations\n",
    "    lda = LdaMallet(PATH_TO_MALLET,\n",
    "                    corpus=corpus,\n",
    "                    id2word=dictionary,\n",
    "                    num_topics=N_TOPICS,\n",
    "                    optimize_interval=10,\n",
    "                    iterations=N_ITERATIONS,\n",
    "                    random_seed = 10)\n",
    "    \n",
    "    return lda\n",
    "\n",
    "def combine_distribution(df, lda, N_TOPICS):\n",
    "    # Combine the distributions to the tweets\n",
    "    joined = df[[\"user_screen_name\", \"tweet_year\", \"clean_text\"]]\n",
    "    joined = joined.reset_index().drop([\"index\"], axis=1)\n",
    "    transformed_docs = lda.load_document_topics()\n",
    "    topic_distributions = pd.DataFrame([[x[1] for x in doc] for doc in transformed_docs], \n",
    "                 columns=['topic_{}'.format(i) for i in range(N_TOPICS)])\n",
    "    topic_distributions.head()\n",
    "    joined = pd.concat([joined, topic_distributions], axis =1, ignore_index=False)\n",
    "\n",
    "    return joined\n",
    "\n",
    "#plot topic distribution by account\n",
    "def topic_user(name,col,topic):\n",
    "    #x=[]\n",
    "    #labels=[]\n",
    "    print(col)\n",
    "    joined_user2 = joined_user.sort_values(col, ascending = False)[:5]\n",
    "    print(plt.bar(joined_user2.index, joined_user2[col]))\n",
    "    #plt.xticks(x, labels)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel('Accounts')\n",
    "    plt.title(name+'\\n Leading accounts for '+topic)\n",
    "    plt.ylabel('Topic probability')\n",
    "    plt.savefig(name+'.png', dpi=600)\n",
    "    plt.show()\n",
    "    \n",
    "#plot wordclouds\n",
    "def wordcloud(name, lda, TOPIC2PLOT, MAX_WORDS):\n",
    "    response = requests.get('https://i.ibb.co/kHNWRYD/black-circle-better.png')\n",
    "    circle_mask = np.array(Image.open(BytesIO(response.content))) \n",
    "    wordcloud = WordCloud(background_color='#fff',\n",
    "                          font_path='/System/Library/Fonts/Supplemental/DIN Alternate Bold.ttf',\n",
    "                        color_func=lambda *args, **kwargs: (0,0,0),\n",
    "                         mask=circle_mask)\n",
    "    wordcloud.generate_from_frequencies(frequencies=dict(lda.show_topic(TOPIC2PLOT, MAX_WORDS)))\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.title('Wordcloud for '+name)\n",
    "    plt.savefig(name+'.png', dpi=600)\n",
    "    \n",
    "    \n",
    "auth_params = {\n",
    "'app_key': \"\",\n",
    "'app_secret': \"\" ,\n",
    "'oauth_token': \"\",\n",
    "'oauth_token_secret':\"\",\n",
    "}\n",
    "adv.twitter.set_auth_params(**auth_params)\n",
    "\n",
    "#get tweets\n",
    "def tweet(user_id):\n",
    "    tweet=adv.twitter.get_user_timeline(user_id=user_id, count=4000, tweet_mode='extended')\n",
    "    tweets=tweet[['user_id','user_name','user_screen_name','user_description','tweet_created_at','tweet_id',\n",
    "                  'tweet_full_text','tweet_lang']]\n",
    "\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3575d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1=get_distance_one_friends(id)\n",
    "data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676e1e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2=get_distance_two_friends(id)\n",
    "data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c175ea27",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_3=get_distance_three_friends(data_2)\n",
    "data_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f890d21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the network\n",
    "part=network(id, name, data_1, data_2, data_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0b51d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the nodes\n",
    "sorted_node = sorted(part.items(), key=operator.itemgetter(1))\n",
    "sorted_dict = OrderedDict()\n",
    "for i, j in sorted_node:\n",
    "    sorted_dict[i] = j\n",
    "    \n",
    "#count nodes number within each cluster    \n",
    "count=Counter(sorted_dict.values())\n",
    "print(count)  \n",
    "print()\n",
    "\n",
    "#print the nodes within same cluster  \n",
    "res = {}\n",
    "for n, m in sorted_dict.items():\n",
    "    res[m] = [n] if m not in res.keys() else res[m] + [n]\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73029c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get tweets from the users in the influence community\n",
    "users=[]#input the id of the users to get their tweets\n",
    "\n",
    "all_tweets=[]\n",
    "for i in users:\n",
    "    tweets=tweet(i)\n",
    "    all_tweets.append(tweets)\n",
    "    \n",
    "all_tweet= pd.concat(all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf56335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user=pd.DataFrame()\n",
    "user[['id','name','screen_name','description']]=all_tweet[['user_id','user_name','user_screen_name',\n",
    "                                                               'user_description']]\n",
    "user=user.drop_duplicates()\n",
    "user['clean_description']=user['description']\n",
    "user['clean_description']=user['clean_description'].str.replace(r'[^\\x00-\\x7F]+', '', regex=True)#remove icons \n",
    "descrip_text=i_user.clean_description.values\n",
    "user['clean_description'] = [clean_text(text) for text in descrip_text]\n",
    "\n",
    "\n",
    "user, dictionary_user, corpus_user= spacy_description(user, nlp_des_nl)\n",
    "\n",
    "user.to_csv(name+'.csv')\n",
    "\n",
    "lda_user=train_LDA(corpus_user, dictionary_user, 1)\n",
    "\n",
    "#wordcloud of users description \n",
    "rcParams[\"figure.figsize\"]=10,5\n",
    "rcParams['axes.titlesize']=12\n",
    "wordcloud(name, lda_user, 0, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9131af",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweet=pre_process(all_tweet)#clean tweet text\n",
    "\n",
    "#seperate tweets into two dataframe(English and Dutch)\n",
    "all_tweet_en=all_tweet.loc[all_tweet['tweet_lang'] == 'en']\n",
    "all_tweet_nl=all_tweet.loc[all_tweet['tweet_lang'] == 'nl']\n",
    "\n",
    "#preprocess tweet text with spaCy \n",
    "all_tweet_en, dictionary_en, corpus_en= spacy(all_tweet_en, nlp_en)\n",
    "all_tweet_nl, dictionary_nl, corpus_nl= spacy(all_tweet_nl, nlp_nl)\n",
    "\n",
    "# save the dataframe\n",
    "all_tweet_en.to_csv(name+'.csv')\n",
    "all_tweet_nl.to_csv(name+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066d8ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the coherence value\n",
    "model_en, coherence_values_en = compute_coherence_values(dictionary=dictionary_en, corpus=corpus_en, \n",
    "                                                             texts=all_tweet_en['tokenized_texts'], start=1, \n",
    "                                                             limit=20, step=3)\n",
    "\n",
    "model_nl, coherence_values_nl = compute_coherence_values(dictionary=dictionary_nl, corpus=corpus_nl, \n",
    "                                                             texts=all_tweet_nl['tokenized_texts'], start=1, \n",
    "                                                             limit=20, step=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556066eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the coherence value for different topic number\n",
    "topic_coherence(name+' community - English tweets', coherence_values_en)\n",
    "topic_coherence(name+' community - Dutch tweets', coherence_values_nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca75d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICS_en=5 #topic number\n",
    "\n",
    "lda_en=train_LDA(corpus_en, dictionary_en, TOPICS_en)\n",
    "\n",
    "for topic in range(TOPICS_en):\n",
    "    words = lda_en.show_topic(topic, topn=10) # get the 10 most relevant words for each topic\n",
    "    topic_n_words = ' '.join([word[0] for word in words])\n",
    "    print('Topic {}: {}'.format(str(topic), topic_n_words))\n",
    "    \n",
    "    \n",
    "joined_en=combine_distribution(all_tweet_en, lda_en, TOPICS_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a001ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall topic distribution per user\n",
    "joined_user = joined_en.groupby('user_screen_name').mean()\n",
    "\n",
    "columns = joined_user.columns\n",
    "\n",
    "#plot the topic distribution for each users within a topic\n",
    "topic_user(name,col,topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409a14dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the topic distribution per user and find the most used topic within the community\n",
    "user_tweet_count=all_tweet_en.user_screen_name.value_counts()\n",
    "user_tweet_count= pd.DataFrame(user_tweet_count)\n",
    "user_topic_dis=joined_en.groupby('user_screen_name').sum()\n",
    "\n",
    "for i in user_topic_dis.columns[1:]:\n",
    "    user_topic_dis[i] = user_topic_dis[i] / user_tweet_count['user_screen_name']#divide the tweet number of the user\n",
    "    \n",
    "for i in user_topic_dis.columns[1:]:\n",
    "    user_topic_dis[i+' sum']=user_topic_dis[i].sum()\n",
    "    \n",
    "print(user_topic_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba26ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud(name+' community \\n English tweets - Topic 0',lda_en, 0, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657c5005",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
